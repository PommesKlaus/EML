\subsection{Lineare Regression}
\label{linearRegression}
Optimale Anpassung einer Geraden an eine gegebene Menge an Punkten, d.h. f체r eine Funktion

\begin{equation*}
    h_{\Theta}(x) = \Theta_0 + \Theta_1x_1 + \dots + \Theta_nx_n
\end{equation*}

soll der Parametervektor $\Theta$ gefunden werden (mit $\Theta_0$ als Konstante), der die Summe der quadrierten Abweichungen der Funktionswerte $h_{\Theta}(x)$ von den tats채chlichen Werten $y$ minimiert (Methode der kleinsten Quadrate):

\begin{equation*}
    \begin{split}
        \underset{\Theta}{\min} L(D, f) = \sum_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2\\
        \underset{\Theta}{\min} L(D, \Theta) = \Vert X_D\Theta-y_D\Vert^2\\
    \end{split}
\end{equation*}

wobei $X_D$ eine Matrix mit den Eingabedaten (zzgl. f체hrende 1-Spalte) ist und $y_D$ der Vektor der tats채chlichen Werte ist:

\begin{equation*}
    \begin{split}
        X_D=
        \begin{pmatrix}
            1 & x_1^{(1)} & \dots & x_n^{(1)}\\
            \vdots & \vdots & \ddots & \vdots\\
            1 & x_1^{(m)} & \dots & x_n^{(m)}\\
        \end{pmatrix},\hspace{2cm}
        y_D=
        \begin{pmatrix}
            y^{(1)}\\
            \vdots\\
            y^{(m)}\\
        \end{pmatrix}
    \end{split}
\end{equation*}

\begin{minted}{python}
    import numpy as np
        
    def incmatrix(genl1,genl2):
        m = len(genl1)
        n = len(genl2)
        M = None #to become the incidence matrix
        VT = np.zeros((n*m,1), int)  #dummy variable
        
        #compute the bitwise xor matrix
        M1 = bitxormatrix(genl1)
        M2 = np.triu(bitxormatrix(genl2),1) 
    
        for i in range(m-1):
            for j in range(i+1, m):
                [r,c] = np.where(M2 == M1[i,j])
                for k in range(len(r)):
                    VT[(i)*n + r[k]] = 1;
                    VT[(i)*n + c[k]] = 1;
                    VT[(j)*n + r[k]] = 1;
                    VT[(j)*n + c[k]] = 1;
                    
                    if M is None:
                        M = np.copy(VT)
                    else:
                        M = np.concatenate((M, VT), 1)
                    
                    VT = np.zeros((n*m,1), int)
        
        return M
    \end{minted}