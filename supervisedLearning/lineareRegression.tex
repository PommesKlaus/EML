\subsection{Lineare Regression}
\label{linearRegression}
Optimale Anpassung einer Geraden an eine gegebene Menge an Punkten, d.h. für eine Funktion

\begin{equation*}
    h_{\Theta}(x) = \Theta_0 + \Theta_1x_1 + \dots + \Theta_nx_n
\end{equation*}

soll der Parametervektor $\Theta$ gefunden werden (mit $\Theta_0$ als Konstante), der die Summe der quadrierten Abweichungen der Funktionswerte $h_{\Theta}(x)$ von den tatsächlichen Werten $y$ minimiert (Methode der kleinsten Quadrate):

\begin{equation*}
    \begin{split}
        \underset{\Theta}{\min} L(D, f) = \min \sum_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2\\
        \underset{\Theta}{\min} L(D, \Theta) = \min \Vert X_D\Theta-y_D\Vert^2\\
    \end{split}
\end{equation*}

wobei $X_D$ eine Matrix mit den Eingabedaten (zzgl. führende 1-Spalte) und $y_D$ der Vektor der tatsächlichen Werte ist:

\begin{equation*}
    \begin{split}
        X_D=
        \begin{pmatrix}
            1 & x_1^{(1)} & \dots & x_n^{(1)}\\
            \vdots & \vdots & \ddots & \vdots\\
            1 & x_1^{(m)} & \dots & x_n^{(m)}\\
        \end{pmatrix},\hspace{2cm}
        y_D=
        \begin{pmatrix}
            y^{(1)}\\
            \vdots\\
            y^{(m)}\\
        \end{pmatrix}
    \end{split}
\end{equation*}

Lokales Minimum = Globales Minimum, da die Kostenfunktion konvex ist. Lösung numerisch oder per \emph{Gradient Descent} $\rightarrow$ ist bei großen Trainingsdatensätzen und/oder vielen Attributen die praktikabelste Methode (s. Skript S.~13: $\nabla_\Theta L(D,\Theta)=0 \Leftrightarrow (X_D^TX_D)^{-1}X_D^Ty_D=\Theta$, wobei inverse von $X_D^TX_D$ sehr rechenaufwändig ist).\\

Evaluation  mittels \textbf{\underline{Bestimmtheitsmaß}} (=normalisierte Variante des quadratischen Fehlers):

\begin{equation*}
    R^2(D,f) = 1 - \frac{\sum_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2}{\sum_{i=1}^{m}(y^{(i)} - \bar{y})^2}
\end{equation*} mit $\bar{y} = \frac{1}{m}\sum_{i=1}^{m}y^{(i)}$, wobei in der Praxis der Durchschnitt mehrerer $R^2$ berechnet wird (\emph{Kreuzvalidierung}).

\begin{itemize}
    \item $R^2(D,f)$  ist maximal 1 $\rightarrow f$ modelliert $D$ perfekt
    \item $R^2(D,f) = 0 \rightarrow$ naives Modell, $f$ sagt stets den Mittelwert $\bar{y}$ voraus
    \item $R^2(D,f) < 0 \rightarrow$ Modell schlechter als naives Modell
    \item $R^2(D^{\text{train}},f)$ sollte relativ nahe an 1 liegen
    \item $R^2(D^{\text{test}},f)$ ist üblicherweise kleiner als $R^2(D^{\text{train}},f)$
    \item Je näher $R^2(D^{\text{test}},f)$ an $R^2(D^{\text{train}},f)$, desto besser ist das Modell generalisiert
\end{itemize}


% \begin{minted}{python}
%     import numpy as np
        
%     def incmatrix(genl1,genl2):
%         m = len(genl1)
%         n = len(genl2)
%         M = None #to become the incidence matrix
%         VT = np.zeros((n*m,1), int)  #dummy variable
        
%         #compute the bitwise xor matrix
%         M1 = bitxormatrix(genl1)
%         M2 = np.triu(bitxormatrix(genl2),1) 
    
%         for i in range(m-1):
%             for j in range(i+1, m):
%                 [r,c] = np.where(M2 == M1[i,j])
%                 for k in range(len(r)):
%                     VT[(i)*n + r[k]] = 1;
%                     VT[(i)*n + c[k]] = 1;
%                     VT[(j)*n + r[k]] = 1;
%                     VT[(j)*n + c[k]] = 1;
                    
%                     if M is None:
%                         M = np.copy(VT)
%                     else:
%                         M = np.concatenate((M, VT), 1)
                    
%                     VT = np.zeros((n*m,1), int)
        
%         return M
%     \end{minted}