\subsection{Lineare/Polynominelle Regression}
\label{linearRegression}
Optimale Anpassung einer Geraden an eine gegebene Menge an Punkten, d.h. für eine Funktion

\begin{equation*}
    h_{\Theta}(x) = \Theta_0 + \Theta_1x_1 + \dots + \Theta_nx_n
\end{equation*}

soll der Parametervektor $\Theta$ gefunden werden (mit $\Theta_0$ als Konstante), der die Summe der quadrierten Abweichungen der Funktionswerte $h_{\Theta}(x)$ von den tatsächlichen Werten $y$ minimiert (Methode der kleinsten Quadrate):

\begin{equation*}
    \begin{split}
        \underset{\Theta}{\min} L(D, f) = \min \sum_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2\\
        \underset{\Theta}{\min} L(D, \Theta) = \min \Vert X_D\Theta-y_D\Vert^2\\
    \end{split}
\end{equation*}

wobei $X_D$ eine Matrix mit den Eingabedaten (zzgl. führende 1-Spalte) und $y_D$ der Vektor der tatsächlichen Werte ist:

\begin{equation*}
    \begin{split}
        X_D=
        \begin{pmatrix}
            1 & x_1^{(1)} & \dots & x_n^{(1)}\\
            \vdots & \vdots & \ddots & \vdots\\
            1 & x_1^{(m)} & \dots & x_n^{(m)}\\
        \end{pmatrix},\hspace{2cm}
        y_D=
        \begin{pmatrix}
            y^{(1)}\\
            \vdots\\
            y^{(m)}\\
        \end{pmatrix}
    \end{split}
\end{equation*}

Lokales Minimum = Globales Minimum, da die Kostenfunktion konvex ist. Lösung numerisch oder per \emph{Gradient Descent} $\rightarrow$ ist bei großen Trainingsdatensätzen und/oder vielen Attributen die praktikabelste Methode (s. Skript S.~13: $\nabla_\Theta L(D,\Theta)=0 \Leftrightarrow (X_D^TX_D)^{-1}X_D^Ty_D=\Theta$, wobei inverse von $X_D^TX_D$ sehr rechenaufwändig ist).\\

Erweiterung auf Polynome höheren Grades durch (Kreuz-)Multiplikation bestehender Merkmale $\rightarrow$ Modell ist linear bzgl. des erweiterten Merkmalsraums und erscheint polynominell bei Projektion auf den ursprünglichen Merkmalsraum.\\

Evaluation  mittels \textbf{\underline{Bestimmtheitsmaß}} (=normalisierte Variante des quadratischen Fehlers):

\begin{equation*}
    R^2(D,f) = 1 - \frac{\sum_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2}{\sum_{i=1}^{m}(y^{(i)} - \bar{y})^2}
\end{equation*} mit $\bar{y} = \frac{1}{m}\sum_{i=1}^{m}y^{(i)}$, wobei in der Praxis der Durchschnitt mehrerer $R^2$ berechnet wird (\emph{Kreuzvalidierung}).

\begin{itemize}
    \item $R^2(D,f)$  ist maximal 1 $\rightarrow f$ modelliert $D$ perfekt
    \item $R^2(D,f) = 0 \rightarrow$ naives Modell, $f$ sagt stets den Mittelwert $\bar{y}$ voraus
    \item $R^2(D,f) < 0 \rightarrow$ Modell schlechter als naives Modell
    \item $R^2(D^{\text{train}},f)$ sollte relativ nahe an 1 liegen
    \item $R^2(D^{\text{test}},f)$ ist üblicherweise kleiner als $R^2(D^{\text{train}},f)$
    \item Je näher $R^2(D^{\text{test}},f)$ an $R^2(D^{\text{train}},f)$, desto besser ist das Modell generalisiert
\end{itemize}

\underline{\textbf{Überanpassung}}: Modell passt sich zu stark an Trainingsdaten an, d.h. es wird zu komplex modelliert. Dies führt zu schlechterer Generalisierung auf Testdaten $\rightarrow$ \emph{Varianzfehler}\\

\underline{\textbf{Unteranpassung}}: Modell ist nicht ausdrucksstark genug; Trainings- und Testdaten werden unzureichend modelliert $\rightarrow$ \emph{Verzerrungsfehler}\\

Ermittlung der \textbf{optimalen Modellkomplexität} durch Betrachtung der Kostenfunktionswerte oder Bestimmtheitsmaße bei steigender Komplexität:
\begin{itemize}
    \item Trainingsdten: Je komplexer das Modell, desto höher die Bestimmtheit
    \item Testdaten
    \begin{itemize}
        \item Bestimmtheit nimmt zunächst ebenfalls zu (das Modell ist noch unterangepasst)
        \item Ab einem gewissen Punkt nimmt die Bestimmtheit ab: Das Modell ist überangepasst
    \end{itemize}
    \item Optimaler Punkt: Modellkomplexität, bei der die Bestimmtheit bzgl. der Testdaten maximal ist
\end{itemize}

Automatische Lösung des \emph{Verzerrungs-Varianz-Dilemmas} durch \textbf{\underline{Regularisierung}}: Hinzufügen eines mit $\lambda$  (\emph{Regularisierungsparamter}) gewichteten Strafterms (\emph{Tikhonov-Regularisierer} $R_T$) zur Kostenfunktion, der die Größe der Parametervektoren begrenzt. Sog. \emph{Ridge-Regression}:\\

\begin{equation*}
    L_T(D, \Theta) = \Vert X_D\Theta-y_D\Vert^2 + \lambda \sum_{\textbf{i=\underline{1}}}^{n}\Theta_i^2\\
\end{equation*}

\begin{itemize}
    \item Regularisierer wird ohne $\Theta_0$ berechnet
    \item Je mehr $\Theta_i \neq 0$, desto größer wird der Tikhonov-Regularisierer $\rightarrow$ Kosten steigend
    \item Einbeziehung von $\lambda \sum_{i=1}^{n}\Theta_i^2$ erzwingt Fokussierung auf möglichst einfache Funktionen
    \item Kleines $\lambda\rightarrow$ Überanpassung, großes $\lambda\rightarrow$ Unteranpassung
\end{itemize}

