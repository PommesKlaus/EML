\subsection{Bayes-Klassifikator}
\label{bayes}

\underline{Optimalität}: Gegeben einem beobachteten Datensatz $D$, suche die wahrscheinlichste Hypothese $h$, die den Wert $P(h\vert D)$ maximiert.\\

\begin{itemize}
    \item \textbf{Bayes Theorem}: $P(h\vert D)=\frac{P(\vert h)P(h)}{P(D)}$
    \item \textbf{a posteriori} Wkt. von $h$: $P(h\vert D)$ $\rightarrow$ kann im allg. \underline{nicht} direkt bestimmt werden
    \item \textbf{a priori} Wkt von $h$: $P(h)$ $\rightarrow$ wie wahrscheinlich ist eine Hypothese $\rightarrow$ Hintergrundwissen oder Annahme von Gleichverteilung
    \item $P(D)$ $\rightarrow$ Wkt, dass $D$ beobachet wurde
    \item $P(D\vert h)$ $\rightarrow$ Wkt, $D$ zu beobachten, gegeben dass $D$ von $h$ \emph{generiert} wurde $\rightarrow$ wie gut erklärt $h$ den Datensatz $D$? Ist einfacher abzuschätzen als $P(h\vert D)$
    \item \textbf{Maximum-A-Posteriori (MAP)}-Hypothese $\rightarrow$ $h* = \text{arg} \underset{h\in H}{\max}P(h\vert D)$
    \item \textbf{Maximum-Likelihood (ML)}-Hypothese $\rightarrow$ $h* = \text{arg} \underset{h\in H}{\max}P(D\vert h)$ \\
\end{itemize}

Es lässt sich zeigen, dass das Problem der \textbf{(linearen) Regression} mit $\Theta* = \text{arg} \min{Theta}\sum_{i=1}^m (\Theta^T)x^{(i)}-y^{(i)})^2$ (d.h. Nutzung der quadratischen Kostenfunktion) eine ML-Hypothese bestimmt, die lineare Regression also nach Bayes'schen Grundsätzen plausibel ist. Dabei wird angenommen, dass die Werte $y{(i)}$ eines Datensatzes $D$ durch den wahren Wert der Merkmalsausprägung $\hat{y}^{(i)}$ und einen Fehler $\epsilon^{(i)}$ bestimmt sind. Bzgl. $\epsilon^{(i)}$ wird üblicherweise Normalverteilung angenommen (Herleitung: Skript S.~59).\\

\textbf{Nachteile} des (normalen) Bayes Klassifikators:
\begin{itemize}
    \item jede mögliche Merkmalsausprägung (Kombination aus mehreren Merkmalen) muss im Trainingsdatensatz vorhanden sein, da Wahrscheinlichkeit sonst undefiniert ($\frac{0}{0}$) ist
    \item der gesamte Datensatz $D$ muss vorgehalten werden $\rightarrow$ Höherer Speicherbedarf und längere Berechnungszeiten
\end{itemize}

Diese Nachteile lassen sich mittels \textbf{Naivem Bayes Klassifikators} lösen. Dieser geht davon aus, dass die einzelnen Merkmalsausprägungen bedingt unabhängig voneinander sind:

\begin{equation*}
    \text{clf}_D^{\text{NaiveBayes}}(x)=\text{arg}\max_{c\in Z} P(c\vert D)P(x_1\vert c, D)P(x_n\vert c, D)\dots P(x_n\vert c, D)
\end{equation*}

\begin{itemize}
    \item $P(c\vert D) \rightarrow$ Für jede Klasse $c$: Wie oft kommt diese in $D$ vor (Wkt)?
    \item $P(x_i\vert c, D) \rightarrow$ Für eine gegebene Klasse $c$: Einzelwkt. für $x_i=\dots$ 
\end{itemize}

Verwendung \emph{kontinuierlicher Merkmale} möglich, indem die direkt aus den Daten berechnete Verteilung $P(x_i\vert c,D)$ durch eine abgeschätzte Dichte $p(x_i\vert c,D)=\frac{1}{\sqrt{2\pi\sigma^2}}e{-\frac{(x_i-\mu)^2}{2\sigma^2}}$ ersetzt wird.