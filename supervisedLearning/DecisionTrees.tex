\subsection{Entscheidungsbäume}
\label{decisionTrees}
Eigenschaften:
\begin{itemize}
    \item Intuitiv und regelbasiert nachvollziehbar
    \item \emph{Entscheidungsbaum} $T$ mit Blattknoten (auch: \emph{Klassifikationsknoten}), inneren Knoten (auch: \emph{Entscheidungsknoten}) und Kanten (E) stellt einen (abwärts) gerichteten Baum mit Wurzel $r$ dar.
    \item Jeder Knoten stellt eine Fallunterscheidung für den Wert eines Merkmals eines gegebenen Datenpunktes dar.
    \item Für jeden Fall gibt es einen eindeutigen Nachfolgeknoten und die Blätter des Baumes enthalten die möglichen Klassifikationen des betrachteten Datenpunkts.
\end{itemize}



Lösung üblicherweise nicht durch Lösen eines Optimierungsproblems sondern durch dedizierte prozedurale Algorithmen wie \textbf{ID3} (Iterative Dichotomiser 3) oder \textbf{C4.5}:
\begin{itemize}
    \item Basieren auf dem Prinzip der \emph{top-down induction of decision trees} (TDIDT) $\rightarrow$ baut Entscheidungsbaum rekursiv von der Wurzel beginnend auf
    \item Abbruchbedingungen $\rightarrow$ alle Beispiele in der betrachteten (Teil-)Menge
    \begin{itemize}
        \item sind von derselben Klasse
        \item haben identische Merkmalsausprägungen $\rightarrow$ wähle am häufigsten vorkommende Klasse
    \end{itemize}
    \item \emph{C4.5-Algorithmus} ist eine Weiterentwicklung von ID3, enthält jedoch folgende Optimierungen:
    \begin{itemize}
        \item ID3 tendiert bei leicht verrauschten Daten zur Überanpassung (d.h. sehr lange Pfade); C4.5 enthält einen Nachverarbeitungsschritt, der weniger relevante Entscheidungsregeln erkennt und den Baum kürzt (sog. \emph{pruning})
        \item bessere Behandlung kontinuierlicher Merkmale
        \item optimierte (skalierte) Version des \emph{Informationsgewinns} (s.u.) $\rightarrow$ unten dargestellte Version bevorzugt Merkmale mit einer hohen Anzahl an Merkmalsausprägungen
    \end{itemize}
\end{itemize}

Merkmal, nach dem die jeweils nächste Aufteilung erfolgt soll so gewählt werden, dass der Baum möglichst klein wird $\rightarrow$ wähle Merkmal mit höchstem Informationsgewinn (IG) unter Verwendung der \emph{Entropie}.
\begin{itemize}
    \item \textbf{Entropie}: $H(D)=-\sum^k_i=1\text{releative Häufigkeit}_i \times \text{log}_10 (\text{releative Häufigkeit}_i) \rightarrow$ je höher $H(D)$, desto gleichverteilter treten die Klassen auf (z.B. $=0$, wenn alle Element einer einzelnen Klasse angehören; dann steht Klassifikation fest).
    \item \textbf{bedingte Entropie}: Was passiert mit der Entropie, wenn ein bestimmtes Merkmal ausgewählt wird? $H(D,i)=\sum \text{rel. Häufigkeit der Merkmalsausprägung} i \times \text{Entropie der Teilmenge} i$
    \item \textbf{Informationsgewinn}: $IG(D, i) = H(D) - H(D\vert i) \rightarrow$ gibt an, wie sehr die Merkmalsauswahl i die Daten in D nach den Klassen (vor-)sortiert. Je höher der Informationsgewinn, desto besser klassifiziert das Merkmal i den Datensatz D
\end{itemize}