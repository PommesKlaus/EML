\subsection{K-Nearest Neighbours}
\label{knn}
\underline{Grundidee}: Klassifizierung eines Objekts anhand der Klassenzugehörigkeit seiner $k$ nächsten Nachbarn (aus $X_\text{Train}$). Bei mehr als $k$ nächsten Nachbarn wird eine zufällige Auswahl der möglichen Kandidaten gewählt. Wenn Klassenzugehörigkeit nicht eindeutig ist (z.B. $k=3$ und alle Elemente haben unterschiedliche Klassen) wird eine zufällige gewählt. Parametrisierung:

\begin{itemize}
    \item Abstandsmessung z.B. per \emph{euklidischer Norm} oder \emph{Manhattan-Norm} $=\sum_{l=1}^{n}|x^i_l-x^j_l|$ (o.a.).
    \item Selektion der Klasse anhand der Mehrheitsentscheidung der $k$ nächsten Nachbarn ($maj$) oder anderen (z.B. mit Gewichtung).
    \item Typische Werte für $k$ liegen im Bereich 1 bis 10, wobei kleinere $k$ zu \emph{Überanpassung} und größere $k$ zu \emph{Unteranpassung} neigen.
\end{itemize}

\underline{\textbf{Regression}}: Statt Klassenzugehörigkeit wird der Mittelwert der $k$ nächsten Nachbarn als Schätzung für den Wert des Objekts verwendet.\\

\underline{\textbf{Vor- und Nachteile}}:
\begin{itemize}
    \item[$+$] Einfach und intuitiv
    \item[$+$] Keine Annahmen über die Verteilung der Daten
    \item[$-$] Langsam bei großen Trainingsdatenmengen
    \item[$-$] Sensibel gegenüber Ausreißern
    \item[$-$] Wahl von $k$ und Abstandsmessung nicht trivial\\
\end{itemize}

\underline{\textbf{Merkmalsskalierung}}: Wichtig bei den meisten ML-Verfahren, da sonst Merkmale mit größeren Werten (Skalen) stärker gewichtet werden (insb. bei Verwendung der Euklidischer Norm). Wichtiger Schritt in der \emph{Datenvorverarbeitung}: Es geht darum Merkmalsausprägungen zu \emph{normieren}. Gebräuchlichste Variante: \emph{z-Transformation} (bzg. \emph{Standardisierung}):

\begin{equation*}
    \text{normiertes Merkmal}\rightarrow\hat{x}^{(i)}_j = \frac{x^{(i)}_j-\bar{x}_j}{\sigma_j}
\end{equation*}
\begin{equation*}
    \text{Mittelwert}\rightarrow\bar{x}_j = \frac{1}{m}\sum_{i=1}^{m}x^{(i)}_j\\
\end{equation*}
\begin{equation*}
    \text{Standardabweichung}\rightarrow\sigma_j = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(x^{(i)}_j-\bar{x}_j)^2}
\end{equation*}\\

$\rightarrow$ Merkmale erhalten eine mittlere Ausprägung von 0 und eine Standardabweichung von 1.\\